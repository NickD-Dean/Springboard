{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93da38b9",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality\n",
    "\n",
    "There are two main drivers of dimensionality in this dataset: Keywords and Production Companies\n",
    "\n",
    "Both bring thousands of dummy variables and create a sparse matrix that must be used for modelling.  I have several other categories that increase dimensionality more than they should, from these I need to severely reduce the number of features that are present in my data.  I'll list the general approach for each here, and enumerate why I'm taking that approach in subsections below. \n",
    "\n",
    "**Keywords:** Eliminate any keyword that appears in less than fifty films, reducing to 23 features\n",
    "\n",
    "**Production Companies:** Bin this by quartiles, reducing to 4 features\n",
    "\n",
    "**Release Year:** Bin by decade, reducing to 6 features\n",
    "\n",
    "**Production Country:** Eliminate and create a single column indicating if a film was produced in the US or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf884ce",
   "metadata": {},
   "source": [
    "Once I've reduced the dimensionality appropriately I'll test the performance of linear, random forest, gradient boosting, xgboost, and k-nearest neighbors regression models. I will also compare model performance using RMSE and R-squared metrics as well as examining the models residual plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09982bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10fd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice = pd.read_csv('../Data/No_Outliers.csv', index_col=0, header=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dde59ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1983, 9976)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copying the boxoffice dataframe prior to making major changes so that I have access to all the information\n",
    "# if any mistakes are made. \n",
    "box = boxoffice\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fd8e5",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "The Keywords category adds 7,134 dimensions to this data set and is incredibly sparse.  Only 1000 keywords actually appear in more than 5 films.  As a starting point I'm going to eliminate all keywords that don't appear in in at least 50 films, which reduces this category by 7,123 dimensions to 11 features. \n",
    "\n",
    "I would prefer to bin keywords by quartiles as I'll do with production companies. This presents problems to stakeholders who would like to make predictions from this model for a film's revenue.  If keywords are binned by revenue, you need to consult the existing list of keywords to identify which quartile a keywords belongs to.  This by itself isn't a problem.  However, over half of all keywords appear in only a single film, and many are a garbled collection of symbols that are not interpretable. A single previous data point is not a good predictor of revenue, further compounded by the likelihood that a keyword has not appeared in a single film previously.  \n",
    "\n",
    "If a keyword has not previously appeared in a film, it's impossible to use prior revenue performance to predict future revenue performance.  As a result, I've chosed to bin keywords by their frequency in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b7f350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Keywords']\n",
    "sums = sums.sum(axis=1)\n",
    "# create a new column that indicates the number of keywords that participate in the creation of a film\n",
    "box['Numerical', 'Num_keywords'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd7c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of keywords that appear in more than 50 films\n",
    "kwrds = boxoffice['Keywords']\n",
    "count = kwrds.apply(pd.value_counts)\n",
    "count = count.iloc[1]\n",
    "pd.DataFrame(count)\n",
    "new_cols = list(count[count>=50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82754da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1983, 2853)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops all keyword columns that don't appear in more than 50 films\n",
    "old_cols = list(box['Keywords'].columns)\n",
    "old_cols = set(old_cols)\n",
    "new_cols = set(new_cols)\n",
    "drop = old_cols.difference(new_cols)\n",
    "drop = list(drop)\n",
    "box.drop(drop, axis=1, level=1, inplace=True)\n",
    "box.drop('No Keywords', axis=1, level=1, inplace=True)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ac276",
   "metadata": {},
   "source": [
    "### Production Companies\n",
    "\n",
    "Next, I need to bin production companies by revenue tier.  This will involve reducing the 2,688 features for Production Companies down to 4 features for production company revenue quartiles. These will indicate if a film has a production company who's fims are generally in the 25th, 50th, 75th, or 100th quartile of film revenue. \n",
    "\n",
    "I'm also going to sum up all the rows for production companies and create a new column that indicates the number of companies that contribute to a film since each film can have multiple companies working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b0ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Company']\n",
    "sums = sums.sum(axis=1)\n",
    "# create a new column that indicates the number of companies that participate in the creation of a film\n",
    "box['Numerical', 'Num_companies'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8247fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the production company data joined with revenue data\n",
    "rev = boxoffice['Numerical', 'revenue']\n",
    "rev = pd.DataFrame(rev)\n",
    "rev.columns = rev.columns.droplevel()\n",
    "prod_co = boxoffice['Company']\n",
    "prod_co_rev = prod_co.join(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d70d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marvel Studios               263427551.00000\n",
       "MVL Incredible Productions   263427551.00000\n",
       "Shinework Media              263036301.00000\n",
       "Taihe Entertainment          263036301.00000\n",
       "Atlantic Television          262520724.00000\n",
       "                                   ...      \n",
       "XM3 Service                              NaN\n",
       "Zehnte Babelsberg                        NaN\n",
       "thinkfilm                                NaN\n",
       "warner bross Turkey                      NaN\n",
       "revenue                                  NaN\n",
       "Length: 2689, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a series that lists companies by the average revenue they produce\n",
    "median={}\n",
    "for x in prod_co_rev:\n",
    "    median[x] = prod_co_rev['revenue'][prod_co_rev[x] == 1].median()\n",
    "medians = pd.Series(median)\n",
    "medians.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e6b98",
   "metadata": {},
   "source": [
    "Interesting that film companies have no revenue. However this would indicate that these companies simply don't have films observed in the data set.  This is not that surprising considering that in the previous notebook I eliminated 5% of the observations.  It is surprising that Warner Bros. is one of those companies with no films present, however it is reasonable that Warner Bros films would be in the top 5% of revenue since it's one of the most well-known companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb28f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that the number of production companies with less than 5%\n",
    "print('Number of film companies with no films: ', medians.isnull().sum())\n",
    "print('% of film companies with no films: ' + str(medians.isnull().sum() / len(medians)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218be5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of production company names to drop later since their films have been dropped from the data set\n",
    "mask = medians.isnull()\n",
    "droplist = list(medians[mask].index)\n",
    "# Edit the series to \n",
    "reverse = medians[medians.notnull()]\n",
    "reverse = reverse.sort_values(ascending=False)\n",
    "reverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24f6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting the list of companies & revenue into quartiles\n",
    "reverse = pd.DataFrame(reverse, columns = ['revenue'])\n",
    "reverse['Quartile'] = pd.qcut(reverse['revenue'], q=4, labels=False)\n",
    "# Creating a list of company names for each quartile\n",
    "quartile_1st = list(reverse[reverse['Quartile'] == 0].index)\n",
    "quartile_2nd = list(reverse[reverse['Quartile'] == 1].index)\n",
    "quartile_3rd = list(reverse[reverse['Quartile'] == 2].index)\n",
    "quartile_4th = list(reverse[reverse['Quartile'] == 3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d31d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested for loops that create new columns that indicate if a film was contributed to by a production company who's average film\n",
    "# revenue is in the 1st, 2nd, 3rd, or 4th quartiles\n",
    "for i in quartile_1st:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_1st')] = 1\n",
    "            \n",
    "for i in quartile_2nd:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_2nd')] = 1\n",
    "        \n",
    "for i in quartile_3rd:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_3rd')] = 1\n",
    "        \n",
    "for i in quartile_4th:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_4th')] = 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a998cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = box['Company']\n",
    "co = co.apply(pd.value_counts)\n",
    "co = co.iloc[1]\n",
    "keep = co[co>50]\n",
    "keep = pd.DataFrame(keep)\n",
    "keep = keep.drop(['Company_quartile_1st', 'Company_quartile_2nd', 'Company_quartile_3rd', 'Company_quartile_4th'], axis=0)\n",
    "keep = set(list(keep.index))\n",
    "drop = set(list(prod_co.columns))\n",
    "drop = list(drop.difference(keep))\n",
    "box = box.drop(drop, axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "box['Company'] = box['Company'].fillna(0)\n",
    "box['Company'] = box['Company'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fcf6f",
   "metadata": {},
   "source": [
    "### Realease Year\n",
    "\n",
    "Generally speaking in the exploratory data analysis for this problem I noticed that revenue increased year over year.  This trend can also be captures by features representing the decade for which a film is produced in.  This will reduce the dimensions of the data set by 41 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e24bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = boxoffice['Release_year']\n",
    "years.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decade_70s = ['1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979']\n",
    "Decade_80s = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989']\n",
    "Decade_90s = ['1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999']\n",
    "Decade_00s = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009']\n",
    "Decade_10s = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
    "for i in Decade_70s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', \"70s\")] = 1\n",
    "for i in Decade_80s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '80s')] = 1\n",
    "for i in Decade_90s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '90s')] = 1\n",
    "for i in Decade_00s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '00s')] = 1\n",
    "for i in Decade_10s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '10s')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ad59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "box = box.drop(['1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979',\n",
    "       '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988',\n",
    "       '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997',\n",
    "       '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006',\n",
    "       '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015',\n",
    "       '2016', '2017'], axis=1, level=1)\n",
    "box['Release_year'] = box['Release_year'].fillna(0)\n",
    "box['Release_year'] = box['Release_year'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c8213",
   "metadata": {},
   "source": [
    "### Country\n",
    "\n",
    "Production country is the last category in this data set that has a sparse matrix of data to work with.  I'm going to group the country information by global region, and I'll also create a new numerical column that indicates if a film was produced in multiple countries.  \n",
    "\n",
    "This will reduce the data set that I'm working with by 55 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Country']\n",
    "sums = sums.sum(axis=1)\n",
    "box['Numerical', 'Num_production_countries'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = boxoffice['Country']\n",
    "europe = ['Austria', 'Belgium', 'Bulgaria', 'Czech Republic', 'Denmark', 'Finland', 'France', 'Germany', 'Greece', \n",
    "          'Hungary', 'Iceland', 'Ireland', 'Italy', 'Luxembourg', 'Malta', 'Monaco', 'Netherlands', 'Norway', 'Poland', \n",
    "          'Portugal', 'Romania', 'Russia', 'Serbia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland']\n",
    "oceania = ['Australia', 'Indonesia', 'New Zealand', 'Philippines']\n",
    "asia = ['Cambodia', 'China', 'Hong Kong', 'Japan', 'Singapore', 'South Korea', 'Taiwan', 'Thailand', 'India']\n",
    "middleast = ['Iran', 'Israel', 'Pakistan', 'Qatar', 'Turkey', 'United Arab Emirates']\n",
    "latinamerica = ['Argentina', 'Bahamas', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Costa Rica', 'Peru', 'Paraguay',\n",
    "                'Uruguay', 'Venezuela']\n",
    "northamerica = ['Canada', 'Mexico']\n",
    "africa = ['Botswana', 'Mauritania', 'Morocco', 'South Africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c931c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in europe:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] == 1:\n",
    "            box.loc[k, ('Country', 'Europe')] = 1\n",
    "for i in oceania:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Oceania')] = 1\n",
    "for i in asia:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Asia')] = 1\n",
    "for i in middleast:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Middle_East')] = 1\n",
    "for i in latinamerica:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Latin_America')] = 1\n",
    "for i in northamerica:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'North_America')] = 1\n",
    "for i in africa:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Africa')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca532b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = europe + oceania + asia + middleast + latinamerica + northamerica + africa\n",
    "box = box.drop(regions, level=1, axis=1)\n",
    "box['Country'] = box['Country'].fillna(0)\n",
    "box['Country'] = box['Country'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b483664",
   "metadata": {},
   "outputs": [],
   "source": [
    "box.to_csv('../Data/Reduced_dimensions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869db2b7",
   "metadata": {},
   "source": [
    "# Testing the performance of regression models on the reduced data set\n",
    "\n",
    "Now that I've severely reduced the dimensionality of this data I'm going to test a linear, random forest, gradient boosting, Support-Vector, and k-nearest neighbors regression models.\n",
    "\n",
    "Based on the results from cross-validation I'll then test out of the box models against the test data and plot their residuals as well as calculate the mean absolute percentage error. \n",
    "\n",
    "### V 1.0 - due to another project and specialized third party packages I'm currently unable to access scikit-learn 0.24 and thus cannot use mean_absolute_percentage_error within cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a929283",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['neg_mean_squared_error', 'r2']\n",
    "# Taking the log of numerical revenue and \n",
    "y = np.log(box['Numerical', 'revenue'])\n",
    "X = box.drop('revenue', level=1, axis=1)\n",
    "# Scaling the numerical data in X using a standard scaler\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X['Numerical'])\n",
    "X['Numerical'] = scaler.transform(X['Numerical'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the regression models\n",
    "linear = LinearRegression()\n",
    "random_forest = RandomForestRegressor()\n",
    "knn = KNeighborsRegressor()\n",
    "gradient_boost = GradientBoostingRegressor()\n",
    "svr = LinearSVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e017dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting linear regression scores\n",
    "results = cross_validate(linear, X_train, y_train, scoring=metrics)\n",
    "# takinge the exponential of the RMSE to transform it back into real dollars\n",
    "RMSE = round(np.exp(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error'])))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "linear_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Linear RMSE: ', RMSE)\n",
    "print('Linear R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d40aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting Random Forest scores\n",
    "results = cross_validate(random_forest, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.exp(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error'])))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "random_forest_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Random Forest RMSE: ', RMSE)\n",
    "print('Random Forest R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting KNN scores\n",
    "results = cross_validate(knn, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.exp(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error'])))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "knn_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('K-Nearest Neighbor RMSE: ', RMSE)\n",
    "print('K-Nearest Neighbor R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Gradient Boosted scores\n",
    "results = cross_validate(gradient_boost, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.exp(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error'])))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "gradient_boosted_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Gradient Boosted RMSE: ', RMSE)\n",
    "print('Gradient Boosted R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Support Vector Regression results\n",
    "results = cross_validate(svr, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.exp(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error'])))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "support_vector_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Support Vector RMSE: ', RMSE)\n",
    "print('Support Vector R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a474b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting results for comparison\n",
    "scores = {'Linear': linear_results, 'Random Forest':random_forest_results, 'KNN':knn_results, \n",
    "          'Gradient Boosted':gradient_boosted_results, 'SVR':support_vector_results}\n",
    "scores = pd.DataFrame(scores)\n",
    "scores = scores.T\n",
    "scores = scores.drop('Linear', axis=0)\n",
    "scores['RMSE'] = scores['RMSE']\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear models are still performing exceptionally poorly - I'm going to drop the data regarding linear model performance\n",
    "models = list(scores.index)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.R2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.axhline(0, c='black')\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('R-squared', size='large')\n",
    "plt.title('R-squared scores by model', size = 'x-large');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.RMSE)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('Root Mean Squared Error ($)', size='large')\n",
    "plt.title('Root Mean Squared Error by Model', size='x-large');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fedf5f",
   "metadata": {},
   "source": [
    "### Testing against test data and plotting residuals\n",
    "\n",
    "Based on cross-validation it seems that decision-tree based regression models as well as KNN regression performs the best.  While the SVR model did recieve better scores, it also raised error warnings that the model is not converging, so I won't be using it going forward.  I'm going to test their performance against my test data as well as take the opportunity to plot their residuals.  \n",
    "\n",
    "***Finally got RMSE that beats many kaggle competition scores!!!!!!!***\n",
    "\n",
    "\n",
    "*In order to re-run randomized cv search simply unhash those lines.  They have been commented out after providing results in order to speed up re-running of this notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fdc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate mean absolute percentage error\n",
    "def MAPE(true, pred):\n",
    "    MAPE = np.sum(np.abs(true-pred/true))/len(true)\n",
    "    return MAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24cbda",
   "metadata": {},
   "source": [
    "#### Testing a Random Forest model, finding optimal parameters, and plotting residuals for the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aece4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "preds = rf.predict(X_test)\n",
    "preds = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c818be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(preds), alpha=0.5, color='g');\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Predicted revenue values')\n",
    "plt.title(\"True vs predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cddc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.exp(preds)-np.exp(y_test)\n",
    "plt.hist(residuals,color='g', bins=50)\n",
    "plt.title('Predicted Revenue vs. Residual Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': np.arange(10,1000), 'min_samples_leaf': np.arange(1,10), 'min_samples_split': np.arange(2,10), \n",
    "          'max_depth':np.arange(10,90)}\n",
    "#rand = RandomizedSearchCV(RandomForestRegressor(), param_distributions=params, n_iter=100)\n",
    "#rand.fit(X_train, y_train)\n",
    "#rf_best_score = rand.best_score_\n",
    "#rf_best_params = rand.best_params_\n",
    "#print(rf_best_score)\n",
    "#print(rf_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c6317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestRegressor(n_estimators= 166, min_samples_split= 5, min_samples_leaf= 3, max_depth= 43)\n",
    "best_rf.fit(X_train, y_train)\n",
    "rf_preds = best_rf.predict(X_test)\n",
    "rf_r2 = r2_score(y_test, rf_preds)\n",
    "rf_rmse = np.exp(mean_squared_error(y_test, rf_preds, squared=False))\n",
    "rf_mape = MAPE(y_test, rf_preds) * 100\n",
    "RandomForest = {'R2':rf_r2, 'RMSE':rf_rmse, 'MAPE':rf_mape}\n",
    "rf_residuals = np.exp(rf_preds) - np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(rf_preds), alpha=0.5, color='b');\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Optimized predicted revenue values')\n",
    "plt.title(\"True vs Optimized predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rf_residuals, color='b', bins=50)\n",
    "plt.title('Distribution of Random Forest Residual Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cf877",
   "metadata": {},
   "source": [
    "### Testing a Gradient Boosted model, finding optimal parameters and plotting residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade = GradientBoostingRegressor()\n",
    "grade.fit(X_train, y_train)\n",
    "preds = grade.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee563e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(preds), alpha=0.5, color='g')\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Predicted revenue values')\n",
    "plt.title(\"True vs predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db781bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.exp(preds)-np.exp(y_test)\n",
    "plt.hist(residuals,color='g', bins=50)\n",
    "plt.title('Distribution of Gradient Boosted Residual Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':np.arange(100,10000), 'max_depth':np.arange(3,90), 'min_samples_split': np.arange(2,10)}\n",
    "#gradient = RandomizedSearchCV(grade, param_distributions=params, n_iter=100)\n",
    "#gradient.fit(X_train, y_train)\n",
    "#grade_best_score = gradient.best_score_\n",
    "#grade_best_params = gradient.best_params_\n",
    "#print('GB best score: ', grade_best_score)\n",
    "#print('GB best parameters: ', grade_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3a7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_gradient=GradientBoostingRegressor(n_estimators= 3955, min_samples_split= 9, max_depth= 8)\n",
    "best_gradient.fit(X_train, y_train)\n",
    "grade_preds = best_gradient.predict(X_test)\n",
    "grade_r2 = r2_score(y_test, grade_preds)\n",
    "grade_rmse = np.exp(mean_squared_error(y_test, grade_preds, squared=False))\n",
    "grade_mape = MAPE(y_test, grade_preds) * 100\n",
    "GradientBoosted = {'R2':grade_r2, 'RMSE':grade_rmse, 'MAPE':grade_mape}\n",
    "gb_residuals = np.exp(grade_preds)-np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f244c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(grade_preds), alpha=0.5, color='b')\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Optimized predicted revenue values')\n",
    "plt.title(\"True vs optimized predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ef097",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gb_residuals, color='g', bins=50)\n",
    "plt.title('Distribution of Gradient Boosted Residual Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2a41c",
   "metadata": {},
   "source": [
    "### Testing a K-Nearest Neighbors model, finding optimal parameters, and plotting residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a1d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor()\n",
    "knn.fit(X_train, y_train)\n",
    "preds = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494210d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(preds), alpha=0.5, color='g')\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Predicted revenue values')\n",
    "plt.title(\"True vs predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f619fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.exp(preds)-np.exp(y_test)\n",
    "plt.hist(residuals,color='g', bins=50)\n",
    "plt.title('Predicted Revenue vs. Residual Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b704af",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_neighbors':np.arange(2,89)}\n",
    "#KNN = RandomizedSearchCV(knn, param_distributions=params, n_iter=87)\n",
    "#KNN.fit(X_train, y_train)\n",
    "#knn_best_score = KNN.best_score_\n",
    "#knn_best_params = KNN.best_params_\n",
    "#print('GB best score: ', knn_best_score)\n",
    "#print('GB best parameters: ', knn_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn = KNeighborsRegressor(n_neighbors= 15)\n",
    "best_knn.fit(X_train, y_train)\n",
    "knn_preds = best_knn.predict(X_test)\n",
    "knn_r2 = r2_score(y_test, knn_preds)\n",
    "knn_rmse = np.exp(mean_squared_error(y_test, knn_preds, squared=False))\n",
    "knn_mape = MAPE(y_test, preds) * 100\n",
    "KNN = {'R2':knn_r2, 'RMSE':knn_rmse, 'MAPE':knn_mape}\n",
    "residuals = np.exp(knn_preds)-np.exp(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc10c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.exp(y_test), np.exp(knn_preds), alpha=0.5, color='b')\n",
    "plt.xlabel('True revenue values')\n",
    "plt.ylabel('Optimized predicted revenue values')\n",
    "plt.title(\"True vs optimized predicted revenue values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0156d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = np.exp(knn_preds)-np.exp(y_test)\n",
    "plt.hist(residuals,color='b', bins=50)\n",
    "plt.title('Predicted Revenue vs. Residual Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313aa4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'Random Forest':RandomForest, 'Gradient Boosting':GradientBoosted, 'K-nearest Neighbors':KNN}\n",
    "opti_scores = pd.DataFrame(dict_)\n",
    "opti_scores = opti_scores.T\n",
    "opti_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec429ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(opti_scores.index)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15, 5))\n",
    "fig.figure\n",
    "ax1.bar(models, opti_scores.R2)\n",
    "ax1.set_title('R-squared scores by Model')\n",
    "ax1.set_xticklabels(models, rotation=45)\n",
    "ax2.bar(models, opti_scores.RMSE)\n",
    "ax2.set_title('RMSE by Model')\n",
    "ax2.set_xticklabels(models, rotation=45)\n",
    "ax3.bar(models, opti_scores.MAPE)\n",
    "ax3.set_title('Mean Absolute Percentage Error by Model')\n",
    "ax3.set_xticklabels(models, rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a0b4a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the results here the random forest regression model performed the best of all of these models, which is surprising.  What is also surprising is that while the Root Mean Squared Error scores for all models is less than 3, the absolute percentage error scores are all within 1% of 1,700%.  This is both higher than the percent error scores recieved on out of the box models, including linear models from previous notebooks.  \n",
    "\n",
    "Because the error is so consistently identical, unlike other scoring metrics, and because the rise in percentage error does not reflect the drop in RMSE score or improvement in R-squared scores; I believe that the percentage error scores are erroneous. Since I wrote that function myself and have yet to make scikit-learn behave and allow me to upgrate to version 0.24 where the MAPE scoring function is provided my time for now is best spent moving to the final section of my project and I can later come back and take a closer look at what is oging on with my percentage error metrics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
