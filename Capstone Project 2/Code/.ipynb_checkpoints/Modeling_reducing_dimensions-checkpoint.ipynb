{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93da38b9",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality\n",
    "\n",
    "There are two main drivers of dimensionality in this dataset: Keywords and Production Companies\n",
    "\n",
    "Both bring thousands of dummy variables and create a sparse matrix that must be used for modelling.  I have several other categories that increase dimensionality more than they should, from these I need to severely reduce the number of features that are present in my data.  I'll list the general approach for each here, and enumerate why I'm taking that approach in subsections below. \n",
    "\n",
    "**Keywords:** Eliminate any keyword that appears in less than fifty films, reducing to 23 features\n",
    "\n",
    "**Production Companies:** Bin this by quartiles, reducing to 4 features\n",
    "\n",
    "**Release Year:** Bin by decade, reducing to 6 features\n",
    "\n",
    "**Production Country:** Eliminate and create a single column indicating if a film was produced in the US or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09982bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99257e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice = pd.read_csv('../Data/No_Outliers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ff407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the boxoffice dataframe prior to making major changes so that I have access to all the information still. \n",
    "box = boxoffice\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090a2de7",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "The Keywords category adds 7,134 dimensions to this data set and is incredibly sparse.  Only 1000 keywords actually appear in more than 5 films.  As a starting point I'm going to eliminate all keywords that don't appear in in at least 50 films, which reduces this category by 7,123 dimensions to 11 features. \n",
    "\n",
    "I would prefer to bin keywords by quartiles as I'll do with production companies. This presents problems to stakeholders who would like to make predictions from this model for a film's revenue.  If keywords are binned by revenue, you need to consult the existing list of keywords to identify which quartile a keywords belongs to.  This by itself isn't a problem.  However, over half of all keywords appear in only a single film, and many are a garbled collection of symbols that are not interpretable. A single previous data point is not a good predictor of revenue, further compounded by the likelihood that a keyword has not appeared in a single film previously.  \n",
    "\n",
    "If a keyword has not previously appeared in a film, it's impossible to use prior revenue performance to predict future revenue performance.  As a result, I've chosed to bin keywords by their frequency in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bda10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwrds = boxoffice['Keywords']\n",
    "count = kwrds.apply(pd.value_counts)\n",
    "count = count.iloc[1]\n",
    "pd.DataFrame(count)\n",
    "new_cols = list(count[count>=50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e2665",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_cols = list(box['Keywords'].columns)\n",
    "old_cols = set(old_cols)\n",
    "new_cols = set(new_cols)\n",
    "drop = old_cols.difference(new_cols)\n",
    "drop = list(drop)\n",
    "box.drop(drop, axis=1, level=1, inplace=True)\n",
    "box.drop('No Keywords', axis=1, level=1, inplace=True)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd092d84",
   "metadata": {},
   "source": [
    "### Production Companies\n",
    "\n",
    "Next, I need to bin production companies by revenue tier.  This will involve reducing the 2,688 features for Production Companies down to 4 features for production company revenue quartiles. These will indicate if a film has a production company who's fims are generally in the 25th, 50th, 75th, or 100th quartile of film revenue. \n",
    "\n",
    "I'm also going to sum up all the rows for production companies and create a new column that indicates the number of companies that contribute to a film since each film can have multiple companies working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Company']\n",
    "sums = sums.sum(axis=1)\n",
    "# create a new column that indicates the number of companies that participate in the creation of a film\n",
    "box['Numerical', 'Num_companies'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46a99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = boxoffice['Numerical', 'revenue']\n",
    "rev = pd.DataFrame(rev)\n",
    "rev.columns = rev.columns.droplevel()\n",
    "prod_co = boxoffice['Company']\n",
    "prod_co_rev = prod_co.join(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33abb6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b48a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175f407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa008d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352373ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
