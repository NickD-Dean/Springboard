{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93da38b9",
   "metadata": {},
   "source": [
    "## Reducing Dimensionality\n",
    "\n",
    "There are two main drivers of dimensionality in this dataset: Keywords and Production Companies\n",
    "\n",
    "Both bring thousands of dummy variables and create a sparse matrix that must be used for modelling.  I have several other categories that increase dimensionality more than they should, from these I need to severely reduce the number of features that are present in my data.  I'll list the general approach for each here, and enumerate why I'm taking that approach in subsections below. \n",
    "\n",
    "**Keywords:** Eliminate any keyword that appears in less than fifty films, reducing to 23 features\n",
    "\n",
    "**Production Companies:** Bin this by quartiles, reducing to 4 features\n",
    "\n",
    "**Release Year:** Bin by decade, reducing to 6 features\n",
    "\n",
    "**Production Country:** Eliminate and create a single column indicating if a film was produced in the US or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf884ce",
   "metadata": {},
   "source": [
    "Once I've reduced the dimensionality appropriately I'll test the performance of linear, random forest, gradient boosting, xgboost, and k-nearest neighbors regression models. I will also compare model performance using RMSE and R-squared metrics as well as examining the models residual plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09982bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d10fd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice = pd.read_csv('../Data/No_Outliers.csv', index_col=0, header=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dde59ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 9976)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copying the boxoffice dataframe prior to making major changes so that I have access to all the information\n",
    "# if any mistakes are made. \n",
    "box = boxoffice\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fd8e5",
   "metadata": {},
   "source": [
    "### Keywords\n",
    "\n",
    "The Keywords category adds 7,134 dimensions to this data set and is incredibly sparse.  Only 1000 keywords actually appear in more than 5 films.  As a starting point I'm going to eliminate all keywords that don't appear in in at least 50 films, which reduces this category by 7,123 dimensions to 11 features. \n",
    "\n",
    "I would prefer to bin keywords by quartiles as I'll do with production companies. This presents problems to stakeholders who would like to make predictions from this model for a film's revenue.  If keywords are binned by revenue, you need to consult the existing list of keywords to identify which quartile a keywords belongs to.  This by itself isn't a problem.  However, over half of all keywords appear in only a single film, and many are a garbled collection of symbols that are not interpretable. A single previous data point is not a good predictor of revenue, further compounded by the likelihood that a keyword has not appeared in a single film previously.  \n",
    "\n",
    "If a keyword has not previously appeared in a film, it's impossible to use prior revenue performance to predict future revenue performance.  As a result, I've chosed to bin keywords by their frequency in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7f350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Keywords']\n",
    "sums = sums.sum(axis=1)\n",
    "# create a new column that indicates the number of keywords that participate in the creation of a film\n",
    "box['Numerical', 'Num_keywords'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd7c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of keywords that appear in more than 50 films\n",
    "kwrds = boxoffice['Keywords']\n",
    "count = kwrds.apply(pd.value_counts)\n",
    "count = count.iloc[1]\n",
    "pd.DataFrame(count)\n",
    "new_cols = list(count[count>=50].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82754da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 2856)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops all keyword columns that don't appear in more than 50 films\n",
    "old_cols = list(box['Keywords'].columns)\n",
    "old_cols = set(old_cols)\n",
    "new_cols = set(new_cols)\n",
    "drop = old_cols.difference(new_cols)\n",
    "drop = list(drop)\n",
    "box.drop(drop, axis=1, level=1, inplace=True)\n",
    "box.drop('No Keywords', axis=1, level=1, inplace=True)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ac276",
   "metadata": {},
   "source": [
    "### Production Companies\n",
    "\n",
    "Next, I need to bin production companies by revenue tier.  This will involve reducing the 2,688 features for Production Companies down to 4 features for production company revenue quartiles. These will indicate if a film has a production company who's fims are generally in the 25th, 50th, 75th, or 100th quartile of film revenue. \n",
    "\n",
    "I'm also going to sum up all the rows for production companies and create a new column that indicates the number of companies that contribute to a film since each film can have multiple companies working on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b0ee7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Company']\n",
    "sums = sums.sum(axis=1)\n",
    "# create a new column that indicates the number of companies that participate in the creation of a film\n",
    "box['Numerical', 'Num_companies'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8247fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe of the production company data joined with revenue data\n",
    "rev = boxoffice['Numerical', 'revenue']\n",
    "rev = pd.DataFrame(rev)\n",
    "rev.columns = rev.columns.droplevel()\n",
    "prod_co = boxoffice['Company']\n",
    "prod_co_rev = prod_co.join(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d70d81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEGO                               468760692.00000\n",
       "Groucho II Film Partnership        463517383.00000\n",
       "thinkfilm                          459359555.00000\n",
       "XM3 Service                        459359555.00000\n",
       "X3US Productions                   459359555.00000\n",
       "                                         ...      \n",
       "The Donners' Company                           NaN\n",
       "Theobald Film Productions                      NaN\n",
       "Vita-Ray Dutch Productions (III)               NaN\n",
       "Warner Bros. Pictures                          NaN\n",
       "revenue                                        NaN\n",
       "Length: 2689, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a series that lists companies by the average revenue they produce\n",
    "median={}\n",
    "for x in prod_co_rev:\n",
    "    median[x] = prod_co_rev['revenue'][prod_co_rev[x] == 1].median()\n",
    "medians = pd.Series(median)\n",
    "medians.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e6b98",
   "metadata": {},
   "source": [
    "Interesting that film companies have no revenue. However this would indicate that these companies simply don't have films observed in the data set.  This is not that surprising considering that in the previous notebook I eliminated 5% of the observations.  It is surprising that Warner Bros. is one of those companies with no films present, however it is reasonable that Warner Bros films would be in the top 5% of revenue since it's one of the most well-known companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb28f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of film companies with no films:  39\n",
      "% of film companies with no films: 0.014503532911863145\n"
     ]
    }
   ],
   "source": [
    "# verifying that the number of production companies with less than 5%\n",
    "print('Number of film companies with no films: ', medians.isnull().sum())\n",
    "print('% of film companies with no films: ' + str(medians.isnull().sum() / len(medians)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "218be5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2650,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a list of production company names to drop later since their films have been dropped from the data set\n",
    "mask = medians.isnull()\n",
    "droplist = list(medians[mask].index)\n",
    "# Edit the series to \n",
    "reverse = medians[medians.notnull()]\n",
    "reverse = reverse.sort_values(ascending=False)\n",
    "reverse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f24f6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting the list of companies & revenue into quartiles\n",
    "reverse = pd.DataFrame(reverse, columns = ['revenue'])\n",
    "reverse['Quartile'] = pd.qcut(reverse['revenue'], q=4, labels=False)\n",
    "# Creating a list of company names for each quartile\n",
    "quartile_1st = list(reverse[reverse['Quartile'] == 0].index)\n",
    "quartile_2nd = list(reverse[reverse['Quartile'] == 1].index)\n",
    "quartile_3rd = list(reverse[reverse['Quartile'] == 2].index)\n",
    "quartile_4th = list(reverse[reverse['Quartile'] == 3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d31d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested for loops that create new columns that indicate if a film was contributed to by a production company who's average film\n",
    "# revenue is in the 1st, 2nd, 3rd, or 4th quartiles\n",
    "for i in quartile_1st:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_1st')] = 1\n",
    "            \n",
    "for i in quartile_2nd:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_2nd')] = 1\n",
    "        \n",
    "for i in quartile_3rd:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_3rd')] = 1\n",
    "        \n",
    "for i in quartile_4th:\n",
    "    for k in prod_co.index:\n",
    "        if prod_co.loc[k,i] == 1:\n",
    "            box.loc[k, ('Company', 'Company_quartile_4th')] = 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3a998cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = box['Company']\n",
    "co = co.apply(pd.value_counts)\n",
    "co = co.iloc[1]\n",
    "keep = co[co>50]\n",
    "keep = pd.DataFrame(keep)\n",
    "keep = keep.drop(['Company_quartile_1st', 'Company_quartile_2nd', 'Company_quartile_3rd', 'Company_quartile_4th'], axis=0)\n",
    "keep = set(list(keep.index))\n",
    "drop = set(list(prod_co.columns))\n",
    "drop = list(drop.difference(keep))\n",
    "box = box.drop(drop, axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c06002f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 185)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box['Company'] = box['Company'].fillna(0)\n",
    "box['Company'] = box['Company'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fcf6f",
   "metadata": {},
   "source": [
    "### Realease Year\n",
    "\n",
    "Generally speaking in the exploratory data analysis for this problem I noticed that revenue increased year over year.  This trend can also be captures by features representing the decade for which a film is produced in.  This will reduce the dimensions of the data set by 41 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1e24bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979',\n",
       "       '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988',\n",
       "       '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997',\n",
       "       '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006',\n",
       "       '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015',\n",
       "       '2016', '2017', 'No Year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = boxoffice['Release_year']\n",
    "years.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0909beb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decade_70s = ['1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979']\n",
    "Decade_80s = ['1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989']\n",
    "Decade_90s = ['1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999']\n",
    "Decade_00s = ['2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009']\n",
    "Decade_10s = ['2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
    "for i in Decade_70s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', \"70s\")] = 1\n",
    "for i in Decade_80s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '80s')] = 1\n",
    "for i in Decade_90s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '90s')] = 1\n",
    "for i in Decade_00s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '00s')] = 1\n",
    "for i in Decade_10s:\n",
    "    for k in years.index:\n",
    "        if years.loc[k,i] == 1:\n",
    "            box.loc[k, ('Release_year', '10s')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "368ad59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 144)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box = box.drop(['1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979',\n",
    "       '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988',\n",
    "       '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997',\n",
    "       '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006',\n",
    "       '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015',\n",
    "       '2016', '2017'], axis=1, level=1)\n",
    "box['Release_year'] = box['Release_year'].fillna(0)\n",
    "box['Release_year'] = box['Release_year'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68c8213",
   "metadata": {},
   "source": [
    "### Country\n",
    "\n",
    "Production country is the last category in this data set that has a sparse matrix of data to work with.  I'm going to group the country information by global region, and I'll also create a new numerical column that indicates if a film was produced in multiple countries.  \n",
    "\n",
    "This will reduce the data set that I'm working with by 55 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0cd0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = boxoffice['Country']\n",
    "sums = sums.sum(axis=1)\n",
    "box['Numerical', 'Num_production_countries'] = sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb6d51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = boxoffice['Country']\n",
    "europe = ['Austria', 'Belgium', 'Bulgaria', 'Czech Republic', 'Denmark', 'Finland', 'France', 'Germany', 'Greece', \n",
    "          'Hungary', 'Iceland', 'Ireland', 'Italy', 'Luxembourg', 'Malta', 'Monaco', 'Netherlands', 'Norway', 'Poland', \n",
    "          'Portugal', 'Romania', 'Russia', 'Serbia', 'Slovenia', 'Spain', 'Sweden', 'Switzerland']\n",
    "oceania = ['Australia', 'Indonesia', 'New Zealand', 'Philippines']\n",
    "asia = ['Cambodia', 'China', 'Hong Kong', 'Japan', 'Singapore', 'South Korea', 'Taiwan', 'Thailand', 'India']\n",
    "middleast = ['Iran', 'Israel', 'Pakistan', 'Qatar', 'Turkey', 'United Arab Emirates']\n",
    "latinamerica = ['Argentina', 'Bahamas', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Costa Rica', 'Peru', 'Paraguay',\n",
    "                'Uruguay', 'Venezuela']\n",
    "northamerica = ['Canada', 'Mexico']\n",
    "africa = ['Botswana', 'Mauritania', 'Morocco', 'South Africa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34c931c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in europe:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] == 1:\n",
    "            box.loc[k, ('Country', 'Europe')] = 1\n",
    "for i in oceania:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Oceania')] = 1\n",
    "for i in asia:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Asia')] = 1\n",
    "for i in middleast:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Middle_East')] = 1\n",
    "for i in latinamerica:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Latin_America')] = 1\n",
    "for i in northamerica:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'North_America')] = 1\n",
    "for i in africa:\n",
    "    for k in country.index:\n",
    "        if country.loc[k, i] ==1:\n",
    "            box.loc[k, ('Country', 'Africa')] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ca532b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2216, 89)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = europe + oceania + asia + middleast + latinamerica + northamerica + africa\n",
    "box = box.drop(regions, level=1, axis=1)\n",
    "box['Country'] = box['Country'].fillna(0)\n",
    "box['Country'] = box['Country'].astype(int)\n",
    "box.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869db2b7",
   "metadata": {},
   "source": [
    "# Testing the performance of regression models on the reduced data set\n",
    "\n",
    "Now that I've severely reduced the dimensionality of this data I'm going to test a linear, random forest, gradient boosting, xgboost, and k-nearest neighbors regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a929283",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['neg_mean_squared_error', 'r2']\n",
    "y = boxoffice['Numerical', 'revenue']\n",
    "X = boxoffice.drop('revenue', level=1, axis=1)\n",
    "# Scaling the numerical data in X using a standard scaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X['Numerical'])\n",
    "X['Numerical'] = scaler.transform(X['Numerical'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb89542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the regression models\n",
    "linear = LinearRegression()\n",
    "random_forest = RandomForestRegressor()\n",
    "knn = KNeighborsRegressor()\n",
    "gradient_boost = GradientBoostingRegressor()\n",
    "svr = LinearSVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e017dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting linear regression scores\n",
    "results = cross-Validate(linear, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error']))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "linear_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Linear RMSE: ', RMSE)\n",
    "print('Linear R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d40aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Random Forest scores\n",
    "results = cross-Validate(random_forest, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error']))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "random_forest_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Random Forest RMSE: ', RMSE)\n",
    "print('Random Forest R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8859cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting KNN scores\n",
    "results = cross-Validate(knn, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error']))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "knn_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('K-Nearest Neighbor RMSE: ', RMSE)\n",
    "print('K-Nearest Neighbor R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Gradient Boosted scores\n",
    "results = cross-Validate(gradient_boost, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error']))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "gradient_boosted_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Gradient Boosted RMSE: ', RMSE)\n",
    "print('Gradient Boosted R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Support Vector Regression results\n",
    "results = cross-Validate(svr, X_train, y_train, scoring=metrics)\n",
    "RMSE = round(np.sqrt(np.mean(np.abs(results['test_neg_mean_squared_error']))), 5)\n",
    "R2 = round(np.mean(results['test_r2']), 5)\n",
    "support_vector_results = {'RMSE':RMSE, 'R2':R2}\n",
    "print('Support Vector RMSE: ', RMSE)\n",
    "print('Support Vector R2: ', R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a474b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting results for comparison\n",
    "scores = {'Linear': linear_results, 'Random Forest':random_forest_results, 'KNN':knn_results, \n",
    "          'Gradient Boosted':gradient_boosted_results, 'SVR':support_vector_results}\n",
    "scores = pd.DataFrame(scores)\n",
    "scores = scores.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.R2)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('R-squared', size='large')\n",
    "plt.title('R-squared scores by model', size = 'x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5c7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.RMSE)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('Root Mean Squared Error ($)', size='large')\n",
    "plt.title('Root Mean Squared Error by Model', size='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98052bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
