{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c5b78d",
   "metadata": {},
   "source": [
    "# Predicting Box Office Revenue\n",
    "\n",
    "## Preprocessing and Training Notebook\n",
    "\n",
    "The purpose of this notebook is twofold:\n",
    "\n",
    "First I want to reduce the dimensionality of my data set in order to make the machine learning models I have to work with more tenable. Currently I have over 10,000 features and only 2,333 observations in the data set, causing serious computational slowdown for my models.  Further, at the time when this notebook was initially written the machine I'm working on didn't have the computational power to even create dummy variables from my 'cast' and 'crew' categories.  \n",
    "\n",
    "\n",
    "I plan to 'bin' categorical dummy variables into new features to reduce dimensionality.  This is relatively easy for features that are highly skewed (spoken language) or where each variable has a low frequency in the data set compared to 'None' (collection).  However I'll need to test performance on a basic linear/polynomial regressor for other categories where the data is exponentially skewed and there isn't a clear way to bin the data.  This will allow me to compare R^2 and Mean Absolute Percent Error metrics and determine which method for dimensionality reduction allows for the more accurate model. \n",
    "\n",
    "Finally, I'll use Lasso and Ridge regressors to test my initial linear regressor for overfit; additionally I'll be able to use the Lasso regressor coefficients to determine what features can be dropped to further reduce dimensionality and complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05043f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f82117e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' (C:\\Users\\deann\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d929d2c51e12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_log_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDummyRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'mean_absolute_percentage_error' from 'sklearn.metrics' (C:\\Users\\deann\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyRegressor\n",
    "import time\n",
    "import sklearn\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e146f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2333, 10456)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in CSV from EDA notebook\n",
    "\n",
    "boxoffice = pd.read_csv('../Data/boxoffice_EDA.csv', index_col=0, header=[0,1])\n",
    "boxoffice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080a6b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">Genre</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Release_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Foreign</th>\n",
       "      <th>...</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 10456 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                                                    \\\n",
       "  Action Adventure Animation Comedy Crime Documentary Drama Family Fantasy   \n",
       "0      0         0         0      1     0           0     1      1       0   \n",
       "1      0         0         0      0     0           0     1      0       0   \n",
       "2      0         1         1      0     0           0     0      1       0   \n",
       "\n",
       "           ... Release_month                             \n",
       "  Foreign  ...             3  4  5  6  7  8  9 10 11 12  \n",
       "0       0  ...             0  0  0  0  0  1  0  0  0  0  \n",
       "1       0  ...             0  0  0  0  0  0  0  1  0  0  \n",
       "2       0  ...             0  0  0  0  0  1  0  0  0  0  \n",
       "\n",
       "[3 rows x 10456 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxoffice.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57db9da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Genre', 'Collection', 'Company', 'Country', 'Spoken_lang', 'Keywords',\n",
       "       'Descriptive', 'Numerical', 'Release_year', 'Release_month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying names of top level of multi-index for later reference\n",
    "boxoffice.columns.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c4776",
   "metadata": {},
   "source": [
    "## Binning Categorical Data\n",
    "\n",
    "To begin I want to significantly cut down on the number of features that I have in the data set.  By far the largest category that I have is Keywords, however the distribution of that category and several others presents several options for binning my data.  I can bin them into quartiles by either frequency or median/mean revenue in the data set. \n",
    "\n",
    "However I do have several feature categories that I won't be touching: Genre, Release_year, Country, and Release_month.  These categories all have fewer than 70 individual features.  Additionally I have a compelling reason to believe that each of these will have a large impact on revenue.  Genre, Release_year, and Release_month all were significantly more evenly distributed across films than other categories.  For these three categories median revenue was also skewed towards certain sub-categories which indicates that they have an impact revenue.  \n",
    "\n",
    "While the Country category is **heavily** skewed towards films made in the United States, revenue is heavily skewed towards more 'exotic' countries.  This is likely a result of blockbuster films like 'Pirates of the Carribean' or 'The Avengers' being filmed on site in other locations.  I suspect that this category will be highly correlated with budget and may be dropped after I check Lasso regresor coefficients.  Regardless, this category only has 67 sub-categories and consolidating those into fewer bins will lose what appears to be useful information with minimal impact on reducing the 10,456 dimensions that the data set currently has.\n",
    "\n",
    "This means that I need to bin the Spoken_lang, Company, Keywords, and Collection columns.  Later I'll come back through and treat the Cast and Crew categories the same as the Keyword category once I have access to a machine with enough RAM to handle those categories. \n",
    "\n",
    "The best place for me to start with binning is going to be the Spoken_lang category since it's highly skewed towards the 'English' sub-category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4066f21",
   "metadata": {},
   "source": [
    "#### Binning Spoken_lang\n",
    "\n",
    "Based on the work from my EDA notebook the 'English' sub-category accounts for 2,180 films in this data set.  It's clear that the simplest way to bin this category is to reduce it to a single column that indicates if a film's primary language is English or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad84127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column lables to be dropped\n",
    "dropped = list(boxoffice['Spoken_lang'].columns)\n",
    "dropped.remove('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b295f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to iterate over the dropped list and remove all languages from Spoken_lang other than english\n",
    "for col in dropped:\n",
    "    boxoffice.drop(col, level=1, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46785a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   English\n",
       "0        1\n",
       "1        1\n",
       "2        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that English is the only remaining column\n",
    "boxoffice.Spoken_lang.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dca544",
   "metadata": {},
   "source": [
    "#### Binning Film Collection dummy variables\n",
    "\n",
    "Ultimately this was going to be relatively complex with the multi-indexed dataframe so I went back to my EDA notebook and created a boolean column that indicates if a film belongs to a collection or not.  The final step is to transform this into the value for 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70b108f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>Collection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Princess Diaries 2: Royal Engagement</td>\n",
       "      <td>Mia Thermopolis is now a college graduate and ...</td>\n",
       "      <td>It can take a lifetime to find true love; she'...</td>\n",
       "      <td>The Princess Diaries 2: Royal Engagement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>Under the direction of a ruthless instructor, ...</td>\n",
       "      <td>The road to greatness can take you to the edge.</td>\n",
       "      <td>Whiplash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pinocchio and the Emperor of the Night</td>\n",
       "      <td>Pinocchio and his friends, a glow worm and a m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinocchio and the Emperor of the Night</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             original_title  \\\n",
       "0  The Princess Diaries 2: Royal Engagement   \n",
       "1                                  Whiplash   \n",
       "2    Pinocchio and the Emperor of the Night   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Mia Thermopolis is now a college graduate and ...   \n",
       "1  Under the direction of a ruthless instructor, ...   \n",
       "2  Pinocchio and his friends, a glow worm and a m...   \n",
       "\n",
       "                                             tagline  \\\n",
       "0  It can take a lifetime to find true love; she'...   \n",
       "1    The road to greatness can take you to the edge.   \n",
       "2                                                NaN   \n",
       "\n",
       "                                      title  Collection  \n",
       "0  The Princess Diaries 2: Royal Engagement           1  \n",
       "1                                  Whiplash           0  \n",
       "2    Pinocchio and the Emperor of the Night           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casting the column in question to a numeric data type\n",
    "boxoffice['Descriptive', 'Collection'] = boxoffice['Descriptive', 'Collection'].astype('int')\n",
    "boxoffice['Descriptive'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6aece",
   "metadata": {},
   "source": [
    "While I'm working on this column in Descriptive I'm also going to drop the overview, tagline, and original_title columns, and set the index to the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff27662",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice.drop('overview', level=1, axis=1, inplace=True)\n",
    "boxoffice.drop('tagline', level=1, axis=1, inplace=True)\n",
    "boxoffice.drop('original_title', level=1, axis=1, inplace=True)\n",
    "# Storing the titles in a seperate Series for later use as needed\n",
    "Titles = boxoffice['Descriptive', 'title']\n",
    "boxoffice.drop('title', level=1, axis=1, inplace=True)\n",
    "boxoffice.drop('Collection', level=0, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dee8e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Collection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Collection\n",
       "0           1\n",
       "1           0\n",
       "2           0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify that the Descriptive category has been reduced to only the one-hot encoded column for collections\n",
    "boxoffice['Descriptive'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86ab605a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">Genre</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Release_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Foreign</th>\n",
       "      <th>...</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 9976 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre                                                                    \\\n",
       "  Action Adventure Animation Comedy Crime Documentary Drama Family Fantasy   \n",
       "0      0         0         0      1     0           0     1      1       0   \n",
       "1      0         0         0      0     0           0     1      0       0   \n",
       "2      0         1         1      0     0           0     0      1       0   \n",
       "\n",
       "           ... Release_month                             \n",
       "  Foreign  ...             3  4  5  6  7  8  9 10 11 12  \n",
       "0       0  ...             0  0  0  0  0  1  0  0  0  0  \n",
       "1       0  ...             0  0  0  0  0  0  0  1  0  0  \n",
       "2       0  ...             0  0  0  0  0  1  0  0  0  0  \n",
       "\n",
       "[3 rows x 9976 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxoffice.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f05de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the modified data frame to a csv - I'll use forward selection to identify if there's a problem with overfitting and \n",
    "# save a further revised data fram if neeeded.\n",
    "boxoffice.to_csv('../Data/boxoffice_PreProcess.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e5a46",
   "metadata": {},
   "source": [
    "## Testing the accuracy of linear regression with the data set as is\n",
    "\n",
    "At this point I have a huge number of columns, and I haven't made significant reductions in the number of features yet. \n",
    "\n",
    "However once I've binned the Company and Keyword columns I'll have eliminated thousands of features.  Prior to doing this I want to get a baseline for how accurate a model is with all of these columns left in the data set. \n",
    "\n",
    "The reason that this is important to do now, prior to reducing the dimensionality of the Company and Keywords categories is that each movie has multiple companies and keywords associated with it, while there is only a single language and film collection for each movie.  Reducing the Spoken_lang and Collection categories isn't eliminating complex information about each film like binning the Company and Keywords categories will be, since each film can have multiple companies or keywords, but only a single film collection or spoken language.\n",
    "\n",
    "Once I have a baseline of performance with the entire dataset using a dummy regressor as well as a general linear regression model with only the numerical data; I'll build up complexity by adding in additional categorical data to assess which provides the best performance. The purpose of building my dataset is to help identify at which point overfitting becomes a problem and decide if, and to what degree, I need to simplify several categories of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28d37f",
   "metadata": {},
   "source": [
    "### Train / Test Split\n",
    "\n",
    "Here I plan to create a train / test split that I'll work with for the remainder of this notebook, and test the accuracy of my best models/data at the end to assess model performance prior to attempting different regression models in the subsequent notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86efbee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the target variable\n",
    "y = boxoffice['Numerical', 'revenue']\n",
    "X = boxoffice.drop('revenue', level=1, axis=1)\n",
    "# Scaling the numerical data in X \n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X['Numerical'])\n",
    "X['Numerical'] = scaler.transform(X['Numerical'])\n",
    "# Creating train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a13bcc",
   "metadata": {},
   "source": [
    "## Checking performance of a dummy regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82508da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.49674973e+08]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reg = DummyRegressor(strategy='mean')\n",
    "mean_reg.fit(X_train, y_train)\n",
    "mean_reg.constant_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0657665",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_absolute_percentage_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9968dbce5dfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Evaluating the mean with mean absolute percent error, R-squared, and root mean squared error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMAPE_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mR2_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mRMSE_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_absolute_percentage_error' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluating the mean with mean absolute percent error, R-squared, and root mean squared error\n",
    "pred = mean_reg.predict(X_train)\n",
    "MAPE_mean = round(mean_absolute_percentage_error(y_train, pred), 5)*100\n",
    "R2_mean = round(r2_score(y_train, pred), 5)\n",
    "RMSE_mean = round(mean_squared_error(y_train, pred, squared=False), 5)\n",
    "print('Mean predictor mape score: ', MAPE_mean)\n",
    "print('Mean predictor r^2 score: ', R2_mean)\n",
    "print('Mean predictor rmse score: ', RMSE_mean)\n",
    "\n",
    "dummy = {'MAPE': MAPE_mean, 'R2':R2_mean, 'RMSE':RMSE_mean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a82f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f33fb1",
   "metadata": {},
   "source": [
    "*This block of code will be reproduced for each iteration of the linear regression model.  At the end of this notebook I'll be able to evaluate the scoring metrics side by side and take note of any patterns.*\n",
    "\n",
    "It seems that the mean revenue is not a strong predictor, which is reasonable.  The smallest value in my target variable is around 5,000 USD, and the largest is over 1.5 Billion USD - a significant jump in revenue to the point where the mean value is actually less than 10% of the largest revenue value.  At the same time the mean revenue for this data set is 2,500% of the smallest value.  There is ***significant*** variability in the target metric for this business problem, which means that I'll need to decide ahead of time what a 'good' metric score will mean.  \n",
    "\n",
    "Given that at it's core, this problem is about predicting human behavior (purchasing of movie tickets), an R-Squared value of 0.5 will be considered 'good'.  This is something that has been cited in multiple blogs online as a good score for predicting human behavior, such as [Jim Frost's blog.](https://statisticsbyjim.com/regression/interpret-r-squared-regression/)\n",
    "\n",
    "Given the huge variability around the mean, and what will be resultingly large mean % error and root-mean-squared errors I think that it's prudent to consider any RMSE score that is less than or equal to one standard deviation as 'good' for now.  Since I'll be saving all of these scores and comparing them at the end of this notebook I'll use that as a standard to make initial judements on as I work to that point.  \n",
    "\n",
    "Mean absolute percentage error is going to be a strong measure of how accurate the model is given the massive scale I'm working with, I'm not sure what I could even call a 'good' MAPE score.  Having run some initial models in a scratch notebook (appended in GitHub), and seeing how the dummy regressor using the mean performs here I believe that any MAPE score less than 1000% should be considered passable, and that anything less than 500% should be considered good. \n",
    "\n",
    "At the end of this notebook I'll be able to identify a subset(s) of my data that will allow a linear regression model to achieve these scores:\n",
    "\n",
    "***R-squared: 0.5***\n",
    "\n",
    "***MAPE: 500%***\n",
    "\n",
    "***RMSE: 176,693,296 USD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a dictionary of metrics to be passed to each cross_validate function using the correct syntax for 'cross_validate'\n",
    "metrics = ['neg_mean_absolute_percentage_error', 'neg_mean_squared_error','r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b20417",
   "metadata": {},
   "source": [
    "## Iterating over versions of the data set with a linear regressor\n",
    "\n",
    "Moving forward I'll be using forward selection to identify which version of my data set provides the best linear regression performance in order to inform what subset of the data is used in my modeling notebook on other types of regression models.  I'll be saving the scoring metrics for each model and comparing those results to inform my approach in the next notebook. \n",
    "\n",
    "I'll forward select different categories in this data set, starting with those that provide the most even distribution of revenue among their members.  In my previous notebook it was clear that revenue is exponentially distributed among the members of each category.  The goal of this process is to identify overfit with the data. As categories get larger the skew of the distribution of revenue becomes more extreme, making forward-selection a good method to use in this instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503312d",
   "metadata": {},
   "source": [
    "### Linear regression with only numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b020913",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_num = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69070f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset the training data on numeric only\n",
    "Num_train = X_train.Numerical\n",
    "Num_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127d2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaving 'cv' as the default 5-folds\n",
    "num_results = cross_validate(linear_num, Num_train, y_train, scoring=metrics)\n",
    "#showing results to more easily create a replicatable method of collecting the mean score for each metric\n",
    "num_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a00c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy mathematical functions to get the mean score for each metric\n",
    "MAPE_num = round(np.mean(np.abs(num_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_num = round(np.sqrt(np.mean(np.abs(num_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_num = round(np.mean(num_results['test_r2']), 5)\n",
    "print('MAPE_num: ', MAPE_num)\n",
    "print('RMSE_num: ', RMSE_num)\n",
    "print('R2_num: ', R2_num)\n",
    "\n",
    "num = {\"MAPE\":MAPE_num, \"RMSE\":RMSE_num, \"R2\":R2_num}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa05bc0",
   "metadata": {},
   "source": [
    "### Linear regression with Numerical & Genre data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_genre = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Spoken_lang', 'Keywords', 'Descriptive', 'Release_year', 'Release_month']\n",
    "Genre = X_train\n",
    "for item in drop_list:\n",
    "    Genre = Genre.drop(item, level=0, axis=1)\n",
    "Genre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010bcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_results = cross_validate(linear_genre, Genre, y_train, scoring=metrics)\n",
    "MAPE_genre = round(np.mean(np.abs(genre_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_genre = round(np.sqrt(np.mean(np.abs(genre_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_genre = round(np.mean(genre_results['test_r2']), 5)\n",
    "print('MAPE_genre: ', MAPE_genre)\n",
    "print('RMSE_genre: ', RMSE_genre)\n",
    "print('R2_genre: ', R2_genre)\n",
    "\n",
    "genre = {'MAPE':MAPE_genre, \"RMSE\":RMSE_genre, 'R2':R2_genre}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e750d",
   "metadata": {},
   "source": [
    "Adding in the categorical data for genres has increased the % error and total dollar error of the model, while increasing R-squared metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da49de",
   "metadata": {},
   "source": [
    "### Linear regression adding in collection information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Spoken_lang', 'Keywords', 'Release_year', 'Release_month']\n",
    "Collection = X_train\n",
    "for item in drop_list:\n",
    "    Collection = Collection.drop(item, level=0, axis=1)\n",
    "Collection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f4571",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_col = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbbc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_results = cross_validate(linear_col, Collection, y_train, scoring=metrics)\n",
    "MAPE_col = round(np.mean(np.abs(col_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_col = round(np.sqrt(np.mean(np.abs(col_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_col = round(np.mean(col_results['test_r2']), 5)\n",
    "print('MAPE_col: ', MAPE_col)\n",
    "print('RMSE_col: ', RMSE_col)\n",
    "print('R2_col: ', R2_col)\n",
    "\n",
    "collection = {\"MAPE\":MAPE_col, \"RMSE\":RMSE_col, \"R2\":R2_col}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296a1c0",
   "metadata": {},
   "source": [
    "Aggain, adding in the boolean for collection information has decreased the % error and RMSE metrics, while increasing the R-squared score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d095d",
   "metadata": {},
   "source": [
    "### Linear regression adding in language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Keywords', 'Release_year', 'Release_month']\n",
    "Lang = X_train\n",
    "for item in drop_list:\n",
    "    Lang = Lang.drop(item, level=0, axis=1)\n",
    "Lang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05cef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_lang = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lang_results = cross_validate(linear_lang, Lang, y_train, scoring=metrics)\n",
    "MAPE_lang = round(np.mean(np.abs(Lang_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_lang = round(np.sqrt(np.mean(np.abs(Lang_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_lang = round(np.mean(Lang_results['test_r2']), 5)\n",
    "print('MAPE_lang: ', MAPE_lang)\n",
    "print('RMSE_lang: ', RMSE_lang)\n",
    "print('R2_lang: ', R2_lang)\n",
    "\n",
    "language = {\"MAPE\":MAPE_lang, \"RMSE\": RMSE_lang, \"R2\":R2_lang}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ebf57",
   "metadata": {},
   "source": [
    "Somewhat surprisingly adding in language information has marginally improved the % error and RMSE scores, **and** improved the R-squared score slightly.  It's notable that all three scores improved here.  In the scratch notebook I was working with prior to this removing categorical information usually improved the MAPE/RMSE **or** R-squared, but not both.   This could be a result of using a different approach (macro-elimination) to test for overfit in that notebook, or perhaps is some interaction between Spoken_lang and the other categories that are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c0904",
   "metadata": {},
   "source": [
    "### Linear regression after adding in data on the release month for each film\n",
    "\n",
    "Note: This is not time-series data, but rather one-hot-encoded features for each month indicating the month that a film was released in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfb7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Keywords', 'Release_year']\n",
    "Month = X_train\n",
    "for item in drop_list:\n",
    "    Month = Month.drop(item, level=0, axis=1)\n",
    "Month.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_month = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092060ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_results = cross_validate(linear_month, Month, y_train, scoring=metrics)\n",
    "MAPE_month = round(np.mean(np.abs(month_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_month = round(np.sqrt(np.mean(np.abs(month_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_month = round(np.mean(month_results['test_r2']), 5)\n",
    "print('MAPE_month: ', MAPE_month)\n",
    "print('RMSE_month: ', RMSE_month)\n",
    "print('R2_month: ', R2_month)\n",
    "\n",
    "month = {\"MAPE\":MAPE_month, \"RMSE\":RMSE_month, \"R2\":R2_month}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cff0fc",
   "metadata": {},
   "source": [
    "Monthly data seems to have improved the MAPE score slightly, however it has equally slightly reduced the RMSE and R-squared scores.  I suspect that this will have a marginal impact on the model performance overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc8b5d8",
   "metadata": {},
   "source": [
    "### Linear regression including data on a film's release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Keywords']\n",
    "Year = X_train\n",
    "for item in drop_list:\n",
    "    Year = Year.drop(item, level=0, axis=1)\n",
    "Year.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33934449",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_year = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f132a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_results = cross_validate(linear_year, Year, y_train, scoring=metrics)\n",
    "MAPE_year = round(np.mean(np.abs(annual_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_year = round(np.sqrt(np.mean(np.abs(annual_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_year = round(np.mean(annual_results['test_r2']), 5)\n",
    "print('MAPE_year: ', MAPE_year)\n",
    "print('RMSE_year: ', RMSE_year)\n",
    "print('R2_year: ', R2_year)\n",
    "\n",
    "annual = {'MAPE':MAPE_year, \"RMSE\":RMSE_year, \"R2\":R2_year}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb4d41",
   "metadata": {},
   "source": [
    "### Linear Regression adding in information on a film's production companies\n",
    "\n",
    "This segment of data is more difficult to bin, since each film has multiple production companies that helped to create it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Country', 'Keywords']\n",
    "Company = X_train\n",
    "for item in drop_list:\n",
    "    Company = Company.drop(item, level=0, axis=1)\n",
    "Company.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_co = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e961b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_results = cross_validate(linear_co, Company, y_train, scoring=metrics)\n",
    "MAPE_co = round(np.mean(np.abs(company_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_co = round(np.sqrt(np.mean(np.abs(company_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_co = round(np.mean(company_results['test_r2']), 5)\n",
    "print('MAPE_co: ', MAPE_co)\n",
    "print('RMSE_co: ', RMSE_co)\n",
    "print('R2_co: ', R2_co)\n",
    "\n",
    "company = {\"MAPE\":MAPE_co, \"RMSE\":RMSE_co, \"R2\":R2_co}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4329f93",
   "metadata": {},
   "source": [
    "### Linear regression after adding in information on the country a film was primarily filmed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ea433",
   "metadata": {},
   "outputs": [],
   "source": [
    "Country = X_train.drop('Keywords', level=0, axis=1)\n",
    "Country.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27732d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_country = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9765bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_results = cross_validate(linear_country, Country, y_train, scoring=metrics)\n",
    "MAPE_country = round(np.mean(np.abs(country_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_country = round(np.sqrt(np.mean(np.abs(country_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_country = round(np.mean(country_results['test_r2']), 5)\n",
    "print('MAPE_country: ', MAPE_country)\n",
    "print('RMSE_country: ', RMSE_country)\n",
    "print('R2_country: ', R2_country)\n",
    "\n",
    "country = {\"MAPE\":MAPE_country, \"RMSE\":RMSE_country, \"R2\":R2_country}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac2d07",
   "metadata": {},
   "source": [
    "### Linear Regression including keyword data.  \n",
    "\n",
    "This is the entire data set I have, and adding in information on keywords increases the dimensions by over 7000. \n",
    "\n",
    "Given the performance of the linear regression model after adding in keyword data to use the full data set, I want to see how a linear regression model performs witholding only the yearly, country, and production company data.  This will let me know if there is some complex interaction between these categories that produces the best model scores, or if the keywords data is able to improve the model scores significantly on it's own.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5144e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset training data appropriately\n",
    "drop_list = ['Company', 'Country', 'Release_year']\n",
    "Ky = X_train\n",
    "for item in drop_list:\n",
    "    Ky = Ky.drop(item, level=0, axis=1)\n",
    "Ky.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_k = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "ky_results = cross_validate(linear_k, Ky, y_train, scoring=metrics)\n",
    "MAPE_ky = round(np.mean(np.abs(ky_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_ky = round(np.sqrt(np.mean(np.abs(ky_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_ky = round(np.mean(ky_results['test_r2']), 5)\n",
    "print('MAPE_ky: ', MAPE_ky)\n",
    "print('RMSE_ky: ', RMSE_ky)\n",
    "print('R2_ky: ', R2_ky)\n",
    "\n",
    "keyword = {\"MAPE\":MAPE_ky, \"RMSE\":RMSE_ky, \"R2\":R2_ky}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be3e1cf",
   "metadata": {},
   "source": [
    "### Linear Regression using the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fba193",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_keywords = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147748d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_results = cross_validate(linear_keywords, X_train, y_train, scoring=metrics)\n",
    "MAPE_key = round(np.mean(np.abs(keyword_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_key = round(np.sqrt(np.mean(np.abs(keyword_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_key = round(np.mean(keyword_results['test_r2']), 5)\n",
    "print('MAPE_key: ', MAPE_key)\n",
    "print('RMSE_key: ', RMSE_key)\n",
    "print('R2_key: ', R2_key)\n",
    "\n",
    "Linear = {\"MAPE\":MAPE_key, \"RMSE\":RMSE_key, \"R2\":R2_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f16236",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_keywords.fit(X_train, y_train)\n",
    "preds = linear_keywords.predict(X_test)\n",
    "print(preds[preds<0])\n",
    "len(preds[preds<0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b25798",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "I want to try out a Lasso regressor on the full dataset to asess if a different model can perform better than expected. \n",
    "\n",
    "Given the high number of dimensions and how lasso coefficients work, it's not possible to asess the efficacy of the full categories using Lasso regression however, rather only individual categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d831c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "lass = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35021ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_results = cross_validate(lass, X_train, y_train, scoring=metrics)\n",
    "MAPE_lasso = round(np.mean(np.abs(lasso_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_lasso = round(np.sqrt(np.mean(np.abs(lasso_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_lasso = round(np.mean(lasso_results['test_r2']), 5)\n",
    "print('MAPE_lasso: ', MAPE_lasso)\n",
    "print('RMSE_lasso: ', RMSE_lasso)\n",
    "print('R2_lasso: ', R2_lasso)\n",
    "\n",
    "lasso = {\"MAPE\":MAPE_lasso, \"RMSE\":RMSE_lasso, \"R2\":R2_lasso}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b9eb3",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Finally, I want to see how a Ridge regressor performs as poorly as a Lasso model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf4ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da3d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_results = cross_validate(ridge, X_train, y_train, scoring=metrics)\n",
    "MAPE_ridge = round(np.mean(np.abs(ridge_results['test_neg_mean_absolute_percentage_error'])), 5)*100\n",
    "RMSE_ridge = round(np.sqrt(np.mean(np.abs(ridge_results['test_neg_mean_squared_error']))), 5)\n",
    "R2_ridge = round(np.mean(ridge_results['test_r2']), 5)\n",
    "print('MAPE_ridge: ', MAPE_ridge)\n",
    "print('RMSE_ridge: ', RMSE_ridge)\n",
    "print('R2_ridge: ', R2_ridge)\n",
    "\n",
    "ridge = {\"MAPE\":MAPE_ridge, \"RMSE\":RMSE_ridge, \"R2\":R2_ridge}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e7e7b",
   "metadata": {},
   "source": [
    "#  Comparing performance metrics for models & subsets of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dict of metrics for each regression model\n",
    "\n",
    "score_metrics = {\"Dummy\":dummy, \"Numerical\":num, \"Genre\":genre, \"Collection\":collection, \"Language\":language, \"Month\":month, \n",
    "                 \"Annual\":annual, \"Company\":company, \"Country\":country, \"Keyword\":keyword, \"Linear\":Linear, \"Lasso\":lasso, \n",
    "                 \"Ridge\":ridge}\n",
    "scores = pd.DataFrame(score_metrics)\n",
    "scores = scores.T\n",
    "models = list(scores.index)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae47d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the Mean Absolute Percentage Error scores\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.MAPE)\n",
    "plt.ylim(0, 3500)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('Mean Average Percent Error', size='large')\n",
    "plt.suptitle('Mean Average Percent Error by Model', size='x-large')\n",
    "plt.title('Annual, Company, Country models have values greater than 10,000, ' \n",
    "          'graph has been limited in scale for readability.', pad=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dfb389",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.R2)\n",
    "plt.ylim(-0.2,0.6)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('R-squared', size='large')\n",
    "plt.suptitle('R_squared score by Model', size='x-large')\n",
    "plt.title('Annual, Company, Country models have R^2 values with a magnitude less than -1000, '\n",
    "         'the plot has been limited in scale for readability.', pad=10)\n",
    "plt.axhline(0, c='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d02c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(models, scores.RMSE)\n",
    "plt.ylim(0, 200000000)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Model', size='large')\n",
    "plt.ylabel('Root Mean Squared Error ($)', size='large')\n",
    "plt.suptitle('Root Mean Squared Error by Model', size='x-large')\n",
    "plt.title('Horizontal line representes 1 STD from the mean film revenue. Four models have errors with a magnitude of $1 sextillion or more, '\n",
    "         'the plot has been limited in scale for readbility.', pad=20)\n",
    "plt.axhline(176693296, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48846662",
   "metadata": {},
   "source": [
    "# Conclusions:\n",
    "\n",
    "Moving forward it seems that the best performing model is a linear model that incorporates the entire data set.  It seems that the massive errors I was getting while using a macro elimination approach to check for overfitting were a result of eliminating one of the four categories that produce significant errors in the model **unless** all four categories are present, in which case the model provides the best metrics possible. \n",
    "\n",
    "This indicates that there is some interesting interaction among these categories which allow them together to provide more predictive power than they do seperately.\n",
    "\n",
    "Of the other models I tested, using only the numerical data from the film's features provided the next best results, and a ridge regression model shows promise.  \n",
    "\n",
    "This will inform my strategy moving forwards for building a model that will accurately predict the revenue of a film based on it's features. \n",
    "\n",
    "I infer that the problems I'm seeing are a result of two things: \n",
    "\n",
    "Outliers in revenue that could be causing both negative predictions on the low end, and creating significant error problems for me with outliers on the high end. \n",
    "\n",
    "Dimensionality, it is without a doubt a huge problem for me.  I have close to 10,000 dimensions in this data frame, and I suspect that it's causing me some serious problems with overfitting, particularly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a0a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
