{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "779dfcd3",
   "metadata": {},
   "source": [
    "# Predicting Box Office Revenue\n",
    "\n",
    "## Preprocessing and Training Notebook\n",
    "\n",
    "The purpose of this notebook is twofold:\n",
    "\n",
    "First I want to reduce the dimensionality of my data set in order to make the machine learning models I have to work with more tenable. Currently I have over 10,000 features and only 2,333 observations in the data set, causing serious computational slowdown for my models.  Further, at the time when this notebook was initially written the machine I'm working on didn't have the computational power to even create dummy variables from my 'cast' and 'crew' categories.  \n",
    "\n",
    "\n",
    "I plan to 'bin' categorical dummy variables into new features to reduce dimensionality.  This is relatively easy for features that are highly skewed (spoken language) or where each variable has a low frequency in the data set compared to 'None' (collection).  However I'll need to test performance on a basic linear/polynomial regressor for other categories where the data is exponentially skewed and there isn't a clear way to bin the data.  This will allow me to compare R^2 and Mean Absolute Percent Error metrics and determine which method for dimensionality reduction allows for the more accurate model. \n",
    "\n",
    "Finally, I'll use Lasso and Ridge regressors to test my initial linear regressor for overfit; additionally I'll be able to use the Lasso regressor coefficients to determine what features can be dropped to further reduce dimensionality and complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e456d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e175c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV from EDA notebook\n",
    "\n",
    "boxoffice = pd.read_csv(r'C:\\Users\\deann\\Documents\\Data\\Box Office Prediction Data\\boxoffice_EDA.csv', index_col=0, \n",
    "                        header=[0,1])\n",
    "\n",
    "boxoffice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dab4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb88edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying names of top level of multi-index for later reference\n",
    "boxoffice.columns.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90191701",
   "metadata": {},
   "source": [
    "## Binning Categorical Data\n",
    "\n",
    "To begin I want to significantly cut down on the number of features that I have in the data set.  By far the largest category that I have is Keywords, however the distribution of that category and several others presents several options for binning my data.  I can bin them into quartiles by either frequency or median/mean revenue in the data set. \n",
    "\n",
    "However I do have several feature categories that I won't be touching: Genre, Release_year, Country, and Release_month.  These categories all have fewer than 70 individual features.  Additionally I have a compelling reason to believe that each of these will have a large impact on revenue.  Genre, Release_year, and Release_month all were significantly more evenly distributed across films than other categories.  For these three categories median revenue was also skewed towards certain sub-categories which indicates that they have an impact revenue.  \n",
    "\n",
    "While the Country category is **heavily** skewed towards films made in the United States, revenue is heavily skewed towards more 'exotic' countries.  This is likely a result of blockbuster films like 'Pirates of the Carribean' or 'The Avengers' being filmed on site in other locations.  I suspect that this category will be highly correlated with budget and may be dropped after I check Lasso regresor coefficients.  Regardless, this category only has 67 sub-categories and consolidating those into fewer bins will lose what appears to be useful information with minimal impact on reducing the 10,456 dimensions that the data set currently has.\n",
    "\n",
    "This means that I need to bin the Spoken_lang, Company, Keywords, and Collection columns.  Later I'll come back through and treat the Cast and Crew categories the same as the Keyword category once I have access to a machine with enough RAM to handle those categories. \n",
    "\n",
    "The best place for me to start with binning is going to be the Spoken_lang category since it's highly skewed towards the 'English' sub-category.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d823f1",
   "metadata": {},
   "source": [
    "#### Binning Spoken_lang\n",
    "\n",
    "Based on the work from my EDA notebook the 'English' sub-category accounts for 2,180 films in this data set.  It's clear that the simplest way to bin this category is to reduce it to a single column that indicates if a film's primary language is English or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column lables to be dropped\n",
    "dropped = list(boxoffice['Spoken_lang'].columns)\n",
    "dropped.remove('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to iterate over the dropped list and remove all languages from Spoken_lang other than english\n",
    "\n",
    "for col in dropped:\n",
    "    boxoffice.drop(col, level=1, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b69f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that English is the only remaining column\n",
    "boxoffice.Spoken_lang.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0525263",
   "metadata": {},
   "source": [
    "#### Binning Film Collection dummy variables\n",
    "\n",
    "Ultimately this was going to be relatively complex with the multi-indexed dataframe so I went back to my EDA notebook and created a boolean column that indicates if a film belongs to a collection or not.  The final step is to transform this into the value for 1 or 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f9b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casting the column in question to a numeric data type\n",
    "#boxoffice['Descriptive', 'Collection'] = boxoffice['Descriptive', 'Collection'].astype('int')\n",
    "#boxoffice['Descriptive', 'Collection'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54307b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice['Descriptive'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88af7016",
   "metadata": {},
   "source": [
    "While I'm working on this column in Descriptive I'm also going to drop the overview, tagline, and original_title columns, and set the index to the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab058022",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice.drop('overview', level=1, axis=1, inplace=True)\n",
    "boxoffice.drop('tagline', level=1, axis=1, inplace=True)\n",
    "boxoffice.drop('original_title', level=1, axis=1, inplace=True)\n",
    "# Storing the titles in a seperate Series for later use as needed\n",
    "Titles = boxoffice['Descriptive', 'title']\n",
    "boxoffice.drop('title', level=1, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c5c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify that the Descriptive category has been reduced to only the one-hot encoded column for collections\n",
    "boxoffice['Descriptive'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b4162",
   "metadata": {},
   "source": [
    "## Testing the accuracy of linear regression with the data set as is\n",
    "\n",
    "At this point I have a huge number of columns, and I haven't made significant reductions in the number of features yet. \n",
    "\n",
    "However once I've binned the Company and Keyword columns I'll have eliminated thousands of features.  Prior to doing this I want to get a baseline for how accurate a model is with all of these columns left in the data set. \n",
    "\n",
    "The reason that this is important to do now, prior to reducing the dimensionality of the Company and Keywords categories is that each movie has multiple companies and keywords associated with it, while there is only a single language and film collection for each movie.  Reducing the Spoken_lang and Collection categories isn't eliminating complex information about each film like binning the Company and Keywords categories will be. \n",
    "\n",
    "This will also give me a baseline of accuracy prior to scaling the numeric data that I have to work with as well.\n",
    "\n",
    "If the model takes excessively long to train I'll be forced to trim a lot of those columns from my data set, so I'll need to take the time required to run and print that as well as test the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2075fe4",
   "metadata": {},
   "source": [
    "#### Renaming columns in Release_month\n",
    "\n",
    "While working with dummy variables I think that it's wise to rename the columns for Release_month since it's possible that I'll be dropping the hierarchical index before creating a train/test split and training my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "months ={'1':'Jan', '2':'Feb', '3':'Mar', '4':'Apr', '5':'May', '6':'June', \n",
    "         '7':'July', '8':'Aug', '9':'Sep', '10':'Oct', '11':'Nov', '12':'Dec'}\n",
    "boxoffice.rename(columns=months, level=1, inplace=True)\n",
    "boxoffice['Release_month'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b41d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxoffice.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad657748",
   "metadata": {},
   "source": [
    "#### Creating the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = boxoffice['Numerical', 'revenue']\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boxoffice.drop('revenue', level=1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e819c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3153d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = datetime.now().time()\n",
    "linear_1.fit(X_train, y_train)\n",
    "train_finish = datetime.now().time()\n",
    "Delta = train_finish-train_start\n",
    "Print('Time to train: ', Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abb035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3602620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78381d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd92a077",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6220a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
